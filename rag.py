# rag.py (ìµœì¢… ë¶„ì„ ê²°ê³¼ë¥¼ Markdown ì„¹ì…˜ìœ¼ë¡œ ì •ë¦¬)
import os
import glob
import time
import requests
import json
from typing import List, Optional, Dict, Any
from config import client, MODEL_RESPONSES

DATA_DIR = "Data"
VSTORE_NAME = "my_pdfs_vector_store"
VS_ID_CACHE = ".vector_store_id"
MODEL = MODEL_RESPONSES
PROMPT_PATH = "prompts/security_rag_baseline.txt"

MCP_URL = "http://localhost:8931/sse"   # Playwright MCP ì„œë²„ (SSE ì§€ì›)


# ===== Baseline Prompt Loader =====
def load_baseline_prompt(path: str = PROMPT_PATH) -> str:
    if not os.path.exists(path):
        return (
            "You are a helpful RAG assistant with a security specialist persona. "
            "Use only the provided context to answer."
        )
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


# ===== Vector Store ê´€ë¦¬ =====
def _load_cached_vs_id() -> Optional[str]:
    if os.path.exists(VS_ID_CACHE):
        return open(VS_ID_CACHE, "r", encoding="utf-8").read().strip() or None
    return None

def _save_cached_vs_id(vs_id: str) -> None:
    with open(VS_ID_CACHE, "w", encoding="utf-8") as f:
        f.write(vs_id)

def _create_vector_store() -> str:
    vs = client.vector_stores.create(name=VSTORE_NAME)
    _save_cached_vs_id(vs.id)
    return vs.id

def _upload_pdfs_to_files_api(pdf_paths: List[str]) -> List[str]:
    file_ids = []
    for p in pdf_paths:
        with open(p, "rb") as f:
            up = client.files.create(file=f, purpose="assistants")
            file_ids.append(up.id)
    return file_ids

def _attach_files_to_vector_store(vs_id: str, file_ids: List[str]) -> None:
    for fid in file_ids:
        client.vector_stores.files.create(vector_store_id=vs_id, file_id=fid)

def _wait_ingestion(vs_id: str, timeout_s: int = 3) -> None:
    time.sleep(timeout_s)


def ensure_vector_store() -> str:
    vs_id = _load_cached_vs_id()
    if vs_id:
        return vs_id

    vs_id = _create_vector_store()
    pdfs = sorted(glob.glob(os.path.join(DATA_DIR, "**/*.pdf"), recursive=True))
    if not pdfs:
        raise RuntimeError(f"Data/ í´ë”ì— PDFê°€ ì—†ìŠµë‹ˆë‹¤: {os.path.abspath(DATA_DIR)}")

    file_ids = _upload_pdfs_to_files_api(pdfs)
    _attach_files_to_vector_store(vs_id, file_ids)
    _wait_ingestion(vs_id)
    return vs_id


# ===== RAG Query (ì •ì /ë™ì  ê²°ê³¼ í†µí•©) =====
def rag_query(url: str, query: str, mcp_results: Optional[Dict[str, Any]] = None, top_k: int = 5) -> str:
    """
    url: ë¶„ì„ ëŒ€ìƒ URL
    query: ì‚¬ìš©ì Prompt
    mcp_results: mcp_llm.pyì—ì„œ ìˆ˜ì§‘ëœ ì •ì /ë™ì  ë¶„ì„ ê²°ê³¼
    """
    vs_id = ensure_vector_store()
    sys_prompt = load_baseline_prompt()

    # 1. RAG ë‹¨ê³„
    rag_resp = client.responses.create(
        model=MODEL,
        input=[
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": f"ëŒ€ìƒ URL: {url}\nì‚¬ìš©ì ìš”ì²­: {query}\n\n"
                                        f"ì´ ìš”ì²­ì„ ë¶„ì„í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì·¨ì•½ì  ì ê²€ ë°©ë²•ì„ ë¬¸ì„œì—ì„œ ì°¾ì•„ë¼."}
        ],
        tools=[{
            "type": "file_search",
            "vector_store_ids": [vs_id],
            "max_num_results": top_k
        }],
    )
    rag_answer = rag_resp.output_text

    # 2. MCP ê²°ê³¼ ìš”ì•½ ì„¹ì…˜ ë§Œë“¤ê¸°
    static_summary = "ì •ì  ë¶„ì„ ê²°ê³¼ ì—†ìŒ"
    dynamic_summary = "ë™ì  ë¶„ì„ ê²°ê³¼ ì—†ìŒ"

    if mcp_results:
        if "urls_sample" in mcp_results or "sites" in mcp_results:
            static_summary = json.dumps({k: mcp_results[k] for k in mcp_results if k in ("sites", "urls_sample")}, 
                                        ensure_ascii=False, indent=2)
        if "alerts" in mcp_results:
            dynamic_summary = json.dumps({"alerts": mcp_results["alerts"]}, ensure_ascii=False, indent=2)

    # 3. ìµœì¢… ìš”ì²­ ë©”ì‹œì§€
    final_input = f"""
# ğŸ” ìµœì¢… ë³´ì•ˆ ë¶„ì„ ë³´ê³ ì„œ

## 1. ì‚¬ìš©ì ìš”ì²­
{query}

## 2. ì •ì  ë¶„ì„ ê²°ê³¼ (Spider ê¸°ë°˜)
{static_summary}

## 3. ë™ì  ë¶„ì„ ê²°ê³¼ (Active Scan ê¸°ë°˜)
{dynamic_summary}

## 4. RAG ê¸°ë°˜ ì·¨ì•½ì  ê°€ì´ë“œ
{rag_answer}

---

ìœ„ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬:
- ëŒ€ìƒ URLì˜ ì·¨ì•½ ì—¬ë¶€ë¥¼ í‰ê°€í•˜ê³ 
- ë°œê²¬ëœ ì·¨ì•½ì ì´ ìˆìœ¼ë©´ ê³µê²© ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì„¤ëª…í•˜ë©°
- êµ¬ì²´ì ì´ê³  ì‹¤ë¬´ì ì¸ íŒ¨ì¹˜ ë°©ë²•ì„ ì œì‹œí•˜ê³ 
- ìš°ì„ ìˆœìœ„ ê¸°ë°˜ì˜ ë³´ì•ˆ ê¶Œê³ ì•ˆì„ ì‘ì„±í•˜ë¼.
"""

    final_resp = client.responses.create(
        model=MODEL,
        input=[
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": final_input}
        ]
    )

    print("\n=== âœ… ìµœì¢… í†µí•© ë¶„ì„ ê²°ê³¼ ===")
    return final_resp.output_text
